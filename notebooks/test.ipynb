{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Librerias comunes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "from src.functions import data_import as dimp\n",
    "from src.functions import data_exploration as dexp\n",
    "from src.functions import data_transformation as dtr\n",
    "\n",
    "# gráficas\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import plotly as pty\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline=True)\n",
    "\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Dónde guardar las imágenes \n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"end_to_end_project\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignorar warnings no útiles (SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algunas pruebas con diferentes modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para establecer un punto de referencia o *benchmark*, probaremos distintos modelos sin profundizar mucho en el tuning de los hiperparámetros. El objetivo es paulatinamente mejorar el rendimiento de los modelos, a base de modificar las fases del proceso (EDA, Feature Engineering, Hyperparameter Tuning, ...).\n",
    "\n",
    "Elegiremos para conformar nuestro set de entrenamiento:\n",
    "* Un solo NWP.\n",
    "* Run de las 00h.\n",
    "* Día D-1.\n",
    "* Variables meteorológicas de partida `time`, `U`, `V` y `T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 29.94 MB\n",
      "Memory usage after optimization is: 7.72 MB\n",
      "Decreased by 74.2%\n",
      "Memory usage of dataframe is 29.26 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\master\\asignaturas\\tfm\\git\\tfm\\src\\functions\\data_import.py:31: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in less\n",
      "\n",
      "d:\\master\\asignaturas\\tfm\\git\\tfm\\src\\functions\\data_import.py:33: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in less\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 19.47 MB\n",
      "Decreased by 33.5%\n",
      "Memory usage of dataframe is 0.57 MB\n",
      "Memory usage after optimization is: 0.21 MB\n",
      "Decreased by 62.5%\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "DATA_PATH = 'D:/Master/Asignaturas/TFM/Git/TFM/data/raw/'\n",
    "X_train = dimp.import_data(os.path.join(DATA_PATH, 'X_train_v2.csv'))\n",
    "X_test = dimp.import_data(os.path.join(DATA_PATH, 'X_test_v2.csv'))\n",
    "Y_train = dimp.import_data(os.path.join(DATA_PATH, 'Y_train.csv'))\n",
    "\n",
    "X_train['Time'] = pd.to_datetime(X_train['Time'], format='%d/%m/%Y %H:%M')\n",
    "X_test['Time'] = pd.to_datetime(X_test['Time'], format='%d/%m/%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>ID</th>\n",
       "      <th>WF</th>\n",
       "      <th>NWP1_00h_D-2_U</th>\n",
       "      <th>NWP1_00h_D-2_V</th>\n",
       "      <th>NWP1_00h_D-2_T</th>\n",
       "      <th>NWP1_06h_D-2_U</th>\n",
       "      <th>NWP1_06h_D-2_V</th>\n",
       "      <th>NWP1_06h_D-2_T</th>\n",
       "      <th>NWP1_12h_D-2_U</th>\n",
       "      <th>...</th>\n",
       "      <th>NWP3_U</th>\n",
       "      <th>NWP3_V</th>\n",
       "      <th>NWP3_T</th>\n",
       "      <th>NWP4_U</th>\n",
       "      <th>NWP4_V</th>\n",
       "      <th>NWP4_CLCT</th>\n",
       "      <th>U</th>\n",
       "      <th>V</th>\n",
       "      <th>T</th>\n",
       "      <th>CLCT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-05-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>WF1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.149414</td>\n",
       "      <td>-2.275391</td>\n",
       "      <td>286.000000</td>\n",
       "      <td>1.254883</td>\n",
       "      <td>-0.289795</td>\n",
       "      <td>82.5625</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>-2.041199</td>\n",
       "      <td>286.250000</td>\n",
       "      <td>82.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-05-01 02:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>WF1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.149414</td>\n",
       "      <td>-2.275391</td>\n",
       "      <td>286.000000</td>\n",
       "      <td>2.490234</td>\n",
       "      <td>-0.413330</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>0.379639</td>\n",
       "      <td>-1.619202</td>\n",
       "      <td>286.125000</td>\n",
       "      <td>100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-05-01 03:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>WF1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.149414</td>\n",
       "      <td>-2.275391</td>\n",
       "      <td>286.000000</td>\n",
       "      <td>0.997070</td>\n",
       "      <td>-1.415039</td>\n",
       "      <td>98.3750</td>\n",
       "      <td>1.456055</td>\n",
       "      <td>-2.273193</td>\n",
       "      <td>285.875000</td>\n",
       "      <td>98.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-05-01 04:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>WF1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.519206</td>\n",
       "      <td>-2.721354</td>\n",
       "      <td>285.666667</td>\n",
       "      <td>0.689453</td>\n",
       "      <td>-0.961426</td>\n",
       "      <td>94.8750</td>\n",
       "      <td>1.763916</td>\n",
       "      <td>-3.171183</td>\n",
       "      <td>285.208333</td>\n",
       "      <td>94.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-05-01 05:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>WF1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111003</td>\n",
       "      <td>-3.167318</td>\n",
       "      <td>285.333333</td>\n",
       "      <td>0.291016</td>\n",
       "      <td>-0.294922</td>\n",
       "      <td>95.8750</td>\n",
       "      <td>1.989990</td>\n",
       "      <td>-3.033040</td>\n",
       "      <td>284.916667</td>\n",
       "      <td>95.8750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Time  ID   WF  NWP1_00h_D-2_U  NWP1_00h_D-2_V  \\\n",
       "0 2018-05-01 01:00:00   1  WF1             NaN             NaN   \n",
       "1 2018-05-01 02:00:00   2  WF1             NaN             NaN   \n",
       "2 2018-05-01 03:00:00   3  WF1             NaN             NaN   \n",
       "3 2018-05-01 04:00:00   4  WF1             NaN             NaN   \n",
       "4 2018-05-01 05:00:00   5  WF1             NaN             NaN   \n",
       "\n",
       "   NWP1_00h_D-2_T  NWP1_06h_D-2_U  NWP1_06h_D-2_V  NWP1_06h_D-2_T  \\\n",
       "0             NaN             NaN             NaN             NaN   \n",
       "1             NaN             NaN             NaN             NaN   \n",
       "2             NaN             NaN             NaN             NaN   \n",
       "3             NaN             NaN             NaN             NaN   \n",
       "4             NaN             NaN             NaN             NaN   \n",
       "\n",
       "   NWP1_12h_D-2_U  ...    NWP3_U    NWP3_V      NWP3_T    NWP4_U    NWP4_V  \\\n",
       "0             NaN  ... -1.149414 -2.275391  286.000000  1.254883 -0.289795   \n",
       "1             NaN  ... -1.149414 -2.275391  286.000000  2.490234 -0.413330   \n",
       "2             NaN  ... -1.149414 -2.275391  286.000000  0.997070 -1.415039   \n",
       "3             NaN  ... -0.519206 -2.721354  285.666667  0.689453 -0.961426   \n",
       "4             NaN  ...  0.111003 -3.167318  285.333333  0.291016 -0.294922   \n",
       "\n",
       "   NWP4_CLCT         U         V           T      CLCT  \n",
       "0    82.5625  0.117188 -2.041199  286.250000   82.5625  \n",
       "1   100.0000  0.379639 -1.619202  286.125000  100.0000  \n",
       "2    98.3750  1.456055 -2.273193  285.875000   98.3750  \n",
       "3    94.8750  1.763916 -3.171183  285.208333   94.8750  \n",
       "4    95.8750  1.989990 -3.033040  284.916667   95.8750  \n",
       "\n",
       "[5 rows x 120 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>ID</th>\n",
       "      <th>WF</th>\n",
       "      <th>NWP1_00h_D-2_U</th>\n",
       "      <th>NWP1_00h_D-2_V</th>\n",
       "      <th>NWP1_00h_D-2_T</th>\n",
       "      <th>NWP1_06h_D-2_U</th>\n",
       "      <th>NWP1_06h_D-2_V</th>\n",
       "      <th>NWP1_06h_D-2_T</th>\n",
       "      <th>NWP1_12h_D-2_U</th>\n",
       "      <th>...</th>\n",
       "      <th>NWP3_U</th>\n",
       "      <th>NWP3_V</th>\n",
       "      <th>NWP3_T</th>\n",
       "      <th>NWP4_U</th>\n",
       "      <th>NWP4_V</th>\n",
       "      <th>NWP4_CLCT</th>\n",
       "      <th>U</th>\n",
       "      <th>V</th>\n",
       "      <th>T</th>\n",
       "      <th>CLCT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-16 01:00:00</td>\n",
       "      <td>37376</td>\n",
       "      <td>WF1</td>\n",
       "      <td>-4.550781</td>\n",
       "      <td>-1.514648</td>\n",
       "      <td>279.00</td>\n",
       "      <td>-3.048828</td>\n",
       "      <td>-2.640625</td>\n",
       "      <td>278.00</td>\n",
       "      <td>-3.363281</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.904297</td>\n",
       "      <td>-0.499268</td>\n",
       "      <td>281.25</td>\n",
       "      <td>-0.701660</td>\n",
       "      <td>-1.819336</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-2.298950</td>\n",
       "      <td>-0.775757</td>\n",
       "      <td>280.250</td>\n",
       "      <td>-0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-16 02:00:00</td>\n",
       "      <td>37377</td>\n",
       "      <td>WF1</td>\n",
       "      <td>-5.792969</td>\n",
       "      <td>0.418701</td>\n",
       "      <td>279.50</td>\n",
       "      <td>-3.115234</td>\n",
       "      <td>-0.492188</td>\n",
       "      <td>277.75</td>\n",
       "      <td>-3.640625</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.904297</td>\n",
       "      <td>-0.499268</td>\n",
       "      <td>281.25</td>\n",
       "      <td>-1.011719</td>\n",
       "      <td>-1.740234</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-2.509277</td>\n",
       "      <td>-0.339233</td>\n",
       "      <td>280.375</td>\n",
       "      <td>-0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-16 03:00:00</td>\n",
       "      <td>37378</td>\n",
       "      <td>WF1</td>\n",
       "      <td>-5.980469</td>\n",
       "      <td>1.007812</td>\n",
       "      <td>280.25</td>\n",
       "      <td>-2.958984</td>\n",
       "      <td>1.196289</td>\n",
       "      <td>277.75</td>\n",
       "      <td>-3.666016</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.904297</td>\n",
       "      <td>-0.499268</td>\n",
       "      <td>281.25</td>\n",
       "      <td>-1.192383</td>\n",
       "      <td>-1.715820</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-2.786865</td>\n",
       "      <td>-0.178101</td>\n",
       "      <td>280.750</td>\n",
       "      <td>-0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-16 04:00:00</td>\n",
       "      <td>37379</td>\n",
       "      <td>WF1</td>\n",
       "      <td>-6.167969</td>\n",
       "      <td>0.498291</td>\n",
       "      <td>280.75</td>\n",
       "      <td>-2.552734</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>278.00</td>\n",
       "      <td>-3.853516</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.343750</td>\n",
       "      <td>-0.311890</td>\n",
       "      <td>281.25</td>\n",
       "      <td>-1.756836</td>\n",
       "      <td>-1.636719</td>\n",
       "      <td>7.855469</td>\n",
       "      <td>-3.447835</td>\n",
       "      <td>-0.235240</td>\n",
       "      <td>281.000</td>\n",
       "      <td>7.855469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-16 05:00:00</td>\n",
       "      <td>37380</td>\n",
       "      <td>WF1</td>\n",
       "      <td>-6.917969</td>\n",
       "      <td>0.222778</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-2.736328</td>\n",
       "      <td>2.484375</td>\n",
       "      <td>278.50</td>\n",
       "      <td>-4.523438</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.783203</td>\n",
       "      <td>-0.124512</td>\n",
       "      <td>281.25</td>\n",
       "      <td>-2.044922</td>\n",
       "      <td>-1.605469</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-4.126628</td>\n",
       "      <td>-0.324346</td>\n",
       "      <td>281.000</td>\n",
       "      <td>-0.000016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Time     ID   WF  NWP1_00h_D-2_U  NWP1_00h_D-2_V  \\\n",
       "0 2019-01-16 01:00:00  37376  WF1       -4.550781       -1.514648   \n",
       "1 2019-01-16 02:00:00  37377  WF1       -5.792969        0.418701   \n",
       "2 2019-01-16 03:00:00  37378  WF1       -5.980469        1.007812   \n",
       "3 2019-01-16 04:00:00  37379  WF1       -6.167969        0.498291   \n",
       "4 2019-01-16 05:00:00  37380  WF1       -6.917969        0.222778   \n",
       "\n",
       "   NWP1_00h_D-2_T  NWP1_06h_D-2_U  NWP1_06h_D-2_V  NWP1_06h_D-2_T  \\\n",
       "0          279.00       -3.048828       -2.640625          278.00   \n",
       "1          279.50       -3.115234       -0.492188          277.75   \n",
       "2          280.25       -2.958984        1.196289          277.75   \n",
       "3          280.75       -2.552734        2.375000          278.00   \n",
       "4          281.00       -2.736328        2.484375          278.50   \n",
       "\n",
       "   NWP1_12h_D-2_U  ...    NWP3_U    NWP3_V  NWP3_T    NWP4_U    NWP4_V  \\\n",
       "0       -3.363281  ... -3.904297 -0.499268  281.25 -0.701660 -1.819336   \n",
       "1       -3.640625  ... -3.904297 -0.499268  281.25 -1.011719 -1.740234   \n",
       "2       -3.666016  ... -3.904297 -0.499268  281.25 -1.192383 -1.715820   \n",
       "3       -3.853516  ... -4.343750 -0.311890  281.25 -1.756836 -1.636719   \n",
       "4       -4.523438  ... -4.783203 -0.124512  281.25 -2.044922 -1.605469   \n",
       "\n",
       "   NWP4_CLCT         U         V        T      CLCT  \n",
       "0  -0.000016 -2.298950 -0.775757  280.250 -0.000016  \n",
       "1  -0.000016 -2.509277 -0.339233  280.375 -0.000016  \n",
       "2  -0.000016 -2.786865 -0.178101  280.750 -0.000016  \n",
       "3   7.855469 -3.447835 -0.235240  281.000  7.855469  \n",
       "4  -0.000016 -4.126628 -0.324346  281.000 -0.000016  \n",
       "\n",
       "[5 rows x 120 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_missing_values(df, cols):\n",
    "    regex = 'NWP(?P<NWP>\\d{1})_(?P<run>\\d{2}h)_(?P<fc_day>D\\W?\\d?)_(?P<weather_var>\\w{1,4})'\n",
    "    p = re.compile(regex)  \n",
    "    \n",
    "    NWP_met_vars_dict = {\n",
    "        '1': ['U','V','T'],\n",
    "        '2': ['U','V'],\n",
    "        '3': ['U','V','T'],\n",
    "        '4': ['U','V','CLCT']\n",
    "    }\n",
    "    \n",
    "    for col in reversed(cols):\n",
    "        m = p.match(col)\n",
    "        col_name = 'NWP' + m.group('NWP') + '_' +  m.group('run') + '_' + m.group('fc_day') + '_' + m.group('weather_var')\n",
    "        nwp = m.group('NWP')\n",
    "\n",
    "        for key, value in NWP_met_vars_dict.items():\n",
    "            for i in value:\n",
    "                if m.group('NWP') == key and m.group('weather_var') == i:\n",
    "                    df['NWP'+ key + '_' + i] = df['NWP'+ key + '_' + i].fillna(df[col_name])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión con KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPE for WF1 is 43.28328915661583\n",
      "Predictions for WF1 has been added to submission_df\n",
      "CAPE for WF2 is 35.6829080057921\n",
      "Predictions for WF2 has been added to submission_df\n",
      "CAPE for WF3 is 38.011376408521606\n",
      "Predictions for WF3 has been added to submission_df\n",
      "CAPE for WF4 is 33.62318687507031\n",
      "Predictions for WF4 has been added to submission_df\n",
      "CAPE for WF5 is 36.27550335648889\n",
      "Predictions for WF5 has been added to submission_df\n",
      "CAPE for WF6 is 33.99157114478972\n",
      "Predictions for WF6 has been added to submission_df\n"
     ]
    }
   ],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train_cpy.columns[3:-11]\n",
    "    cols_test = X_test_cpy.columns[3:-11]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_train)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U + X_train_cpy.NWP4_U)/4\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V + X_train_cpy.NWP4_V)/4\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = (X_train_cpy.NWP4_CLCT)\n",
    "    X_train_cpy['inv_T'] = 1/(X_train_cpy['T'])\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = (X_test_cpy.NWP4_CLCT)\n",
    "    X_test_cpy['inv_T'] = 1/(X_test_cpy['T'])\n",
    "\n",
    "    \n",
    "    \n",
    "    X_train_cpy = X_train_cpy[['Time','U','V','T','CLCT','inv_T']]\n",
    "    X_test_cpy = X_test_cpy[['Time','U','V','T','CLCT','inv_T']]\n",
    "\n",
    "    \n",
    "    ## Limpieza de datos: imputar valores perdidos en X_\n",
    "    #only_na = X_test_cpy[~X_test_cpy.ID.isin(X_test_cpy.dropna().ID)]\n",
    "    #X_test_cpy.dropna(inplace=True)\n",
    "    #Y_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "    #X_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', \n",
    "                                                     ['Time','U','V','T','CLCT'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.DerivedAttributesAdder(add_time_feat=False, add_cyclic_feat=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: KNN\n",
    "    knn_reg = KNeighborsRegressor()\n",
    "    param_grid = [\n",
    "        {\n",
    "            'n_neighbors': list(range(1,30)),\n",
    "            'algorithm':['auto', 'kd_tree'],\n",
    "            'weights': ['uniform','distance'],\n",
    "            'p': [1,2]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    grid_search_knn = GridSearchCV(\n",
    "        knn_reg, \n",
    "        param_grid, \n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        scoring=cape_scorer\n",
    "    )\n",
    "\n",
    "    grid_search_knn.fit(X_train_pped, Y_train_cpy)\n",
    "    final_model = grid_search_knn.best_estimator_\n",
    "    \n",
    "    # guardamos modelo\n",
    "    models.append(joblib.dump(final_model, WF + '_knn'))\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    predictions = final_model.predict(X_test_pped)\n",
    "    \n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('CAPE for {} is {}'.format(WF, -grid_search_knn.best_score_))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/submission_knn.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 21, 'p': 1, 'weights': 'uniform'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_knn.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 1: Regresión polinomial con regularización *ridge*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "    X_train_cpy = X_train_cpy[['Time','NWP1_00h_D-1_U','NWP1_00h_D-1_V','NWP1_00h_D-1_T']]\n",
    "    X_train_cpy.rename(\n",
    "        columns={\"Time\": \"time\", \"NWP1_00h_D-1_U\": \"U\", \"NWP1_00h_D-1_V\": \"V\", \"NWP1_00h_D-1_T\":\"T\"}, \n",
    "        inplace=True)\n",
    "\n",
    "    X_test_cpy = X_test_cpy[['Time','NWP1_00h_D-1_U','NWP1_00h_D-1_V','NWP1_00h_D-1_T']]\n",
    "    X_test_cpy.rename(\n",
    "        columns={\"Time\": \"time\", \"NWP1_00h_D-1_U\": \"U\", \"NWP1_00h_D-1_V\": \"V\", \"NWP1_00h_D-1_T\":\"T\"}, \n",
    "        inplace=True)\n",
    "\n",
    "    # Limpieza de datos: eliminar valores perdidos\n",
    "    only_na = X_train_cpy[~X_train_cpy.index.isin(X_train_cpy.dropna().index)]\n",
    "    X_train_cpy.dropna(inplace=True)\n",
    "    Y_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "\n",
    "    X_test_cpy.dropna(inplace=True)\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop',\n",
    "                                                     ['time','U','V','month','hour','day_of_week'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.DerivedAttributesAdder()), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Regresión polinomial utilizando CV con regularización tipo Ride\n",
    "    poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "    X_train_poly = poly_features.fit_transform(X_train_pped)\n",
    "    \n",
    "    rreg = lm.RidgeCV(alphas=np.logspace(-4, -3, 3, 4), store_cv_values=True)\n",
    "    rreg.fit(X_train_poly, Y_train_cpy)\n",
    "    \n",
    "    # guardamos modelo\n",
    "    models.append(joblib.dump(rreg, WF + '_rreg'))\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    X_test_poly = poly_features.fit_transform(X_test_pped)\n",
    "    predictions = rreg.predict(X_test_poly)\n",
    "    \n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('RMSE for {} is {}'.format(WF, np.sqrt(rreg.cv_values_)))\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"./Data/submission_rreg.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 2: Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.externals import joblib\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 5)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train_cpy.columns[3:-11]\n",
    "    cols_test = X_test_cpy.columns[3:-11]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_train)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U + X_train_cpy.NWP4_U)/4\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V + X_train_cpy.NWP4_V)/4\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = (X_train_cpy.NWP4_CLCT)\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = (X_test_cpy.NWP4_CLCT)\n",
    "\n",
    "    \n",
    "    \n",
    "    X_train_cpy = X_train_cpy[['Time','U','V','T','CLCT']]\n",
    "    X_test_cpy = X_test_cpy[['Time','U','V','T','CLCT']]\n",
    "\n",
    "    \n",
    "    ## Limpieza de datos: imputar valores perdidos en X_\n",
    "    #only_na = X_test_cpy[~X_test_cpy.ID.isin(X_test_cpy.dropna().ID)]\n",
    "    #X_test_cpy.dropna(inplace=True)\n",
    "    #Y_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "    #X_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', \n",
    "                                                     ['Time','U','V'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.DerivedAttributesAdder(add_time_feat=True, add_cyclic_feat=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "\n",
    "    # Modelización: Random Forest\n",
    "    # Entrenamiento del modelo mediante k-fold cross validation\n",
    "    # Búsqueda de hiperparámetros mediante Gridsearch \n",
    "    # param_grid = [\n",
    "    #    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    #    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "    #]\n",
    "\n",
    "    forest_reg = RandomForestRegressor()\n",
    "    grid_search = GridSearchCV(\n",
    "        forest_reg, \n",
    "        random_grid, \n",
    "        cv=5,\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train_pped, Y_train_cpy)\n",
    "    final_model = grid_search.best_estimator_\n",
    "\n",
    "    # guardamos modelo\n",
    "    #models.append(joblib.dump(final_model, WF))\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    predictions = final_model.predict(X_test_pped)\n",
    "\n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True)  \n",
    "    print('Best score for {}: {}'.format(WF, -grid_search.best_score_))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('--------')\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/submission_RF.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.77150000e+04, 8.24496733e-02],\n",
       "       [6.77160000e+04, 9.10754914e-02],\n",
       "       [6.77170000e+04, 2.03519919e-02],\n",
       "       ...,\n",
       "       [7.39020000e+04, 2.40697737e-01],\n",
       "       [7.39030000e+04, 1.21634805e-01],\n",
       "       [7.39040000e+04, 2.31507012e-01]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 3: SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.externals import joblib\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "\n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train_cpy.columns[3:-11]\n",
    "    cols_test = X_test_cpy.columns[3:-11]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_train)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U + X_train_cpy.NWP4_U)/4\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V + X_train_cpy.NWP4_V)/4\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = (X_train_cpy.NWP4_CLCT)\n",
    "    X_train_cpy['inv_T'] = 1/(X_train_cpy['T'])\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = (X_test_cpy.NWP4_CLCT)\n",
    "    X_test_cpy['inv_T'] = 1/(X_test_cpy['T'])\n",
    "\n",
    "    \n",
    "    \n",
    "    X_train_cpy = X_train_cpy[['Time','U','V','T','CLCT']]\n",
    "    X_test_cpy = X_test_cpy[['Time','U','V','T','CLCT']]\n",
    "\n",
    "    \n",
    "    ## Limpieza de datos: imputar valores perdidos en X_\n",
    "    #only_na = X_test_cpy[~X_test_cpy.ID.isin(X_test_cpy.dropna().ID)]\n",
    "    #X_test_cpy.dropna(inplace=True)\n",
    "    #Y_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "    #X_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', \n",
    "                                                     ['Time','U','V','T'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.DerivedAttributesAdder(add_time_feat=False, add_cyclic_feat=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "\n",
    "    # Modelización: Random Forest\n",
    "    # Entrenamiento del modelo mediante k-fold cross validation\n",
    "    # Búsqueda de hiperparámetros mediante Gridsearch \n",
    "    param_grid = [\n",
    "        {\n",
    "            'kernel': ('linear', 'rbf','poly'), \n",
    "            'C':[0.001, 0.1, 10],\n",
    "            'gamma': [0.001, 0.1, 10]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    svm_reg = SVR()\n",
    "    grid_search_svm = GridSearchCV(\n",
    "        svm_reg, \n",
    "        param_grid, \n",
    "        cv=5,\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid_search_svm.fit(X_train_pped, Y_train_cpy)\n",
    "    final_model = grid_search_svm.best_estimator_\n",
    "\n",
    "    # guardamos modelo\n",
    "    # models.append(joblib.dump(final_model, WF + '_SVM'))\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    predictions = final_model.predict(X_test_pped)\n",
    "\n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('Best score for {}: {}'.format(WF, -grid_search_svm.best_score_))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('--------')\n",
    "        \n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/submission_SVM.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 4: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train_cpy.columns[3:-11]\n",
    "    cols_test = X_test_cpy.columns[3:-11]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_train)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U + X_train_cpy.NWP4_U)/4\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V + X_train_cpy.NWP4_V)/4\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = (X_train_cpy.NWP4_CLCT)\n",
    "    X_train_cpy['inv_T'] = 1/(X_train_cpy['T'])\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = (X_test_cpy.NWP4_CLCT)\n",
    "    X_test_cpy['inv_T'] = 1/(X_test_cpy['T'])\n",
    "\n",
    "    \n",
    "    \n",
    "    X_train_cpy = X_train_cpy[['Time','U','V','T','CLCT','inv_T']]\n",
    "    X_test_cpy = X_test_cpy[['Time','U','V','T','CLCT','inv_T']]\n",
    "\n",
    "    \n",
    "    ## Limpieza de datos: imputar valores perdidos en X_\n",
    "    #only_na = X_test_cpy[~X_test_cpy.ID.isin(X_test_cpy.dropna().ID)]\n",
    "    #X_test_cpy.dropna(inplace=True)\n",
    "    #Y_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "    #X_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', \n",
    "                                                     ['Time','U','V','T','CLCT'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.DerivedAttributesAdder(add_time_feat=False, add_cyclic_feat=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Random Forest\n",
    "    # Entrenamiento del modelo mediante k-fold cross validation\n",
    "    # Búsqueda de hiperparámetros mediante Gridsearch \n",
    "    param_grid = [{\n",
    "        'n_estimators': [400, 700, 1000],\n",
    "        'colsample_bytree': [0.7, 0.8],\n",
    "        'max_depth': [15,20,25],\n",
    "        'reg_alpha': [1.1, 1.2, 1.3],\n",
    "        'reg_lambda': [1.1, 1.2, 1.3],\n",
    "        'subsample': [0.7, 0.8, 0.9]\n",
    "    }]\n",
    "\n",
    "    xgb_reg = xgb.XGBRegressor()\n",
    "    grid_search_xgb = GridSearchCV(\n",
    "        xgb_reg, \n",
    "        param_grid, \n",
    "        cv=5,\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid_search_xgb.fit(X_train_pped, Y_train_cpy)\n",
    "    final_model = grid_search_xgb.best_estimator_\n",
    "\n",
    "    # guardamos modelo\n",
    "    models.append(joblib.dump(final_model, WF + '_xgb'))\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    predictions = final_model.predict(X_test_pped)\n",
    "\n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "        \n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/submission_xgb.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 5: Random Forest con validación Randomized Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "    X_train_cpy = X_train_cpy[['Time','NWP1_00h_D-1_U','NWP1_00h_D-1_V','NWP1_00h_D-1_T']]\n",
    "    X_train_cpy.rename(\n",
    "        columns={\"Time\": \"time\", \"NWP1_00h_D-1_U\": \"U\", \"NWP1_00h_D-1_V\": \"V\", \"NWP1_00h_D-1_T\":\"T\"}, \n",
    "        inplace=True)\n",
    "\n",
    "    X_test_cpy = X_test_cpy[['Time','NWP1_00h_D-1_U','NWP1_00h_D-1_V','NWP1_00h_D-1_T']]\n",
    "    X_test_cpy.rename(\n",
    "        columns={\"Time\": \"time\", \"NWP1_00h_D-1_U\": \"U\", \"NWP1_00h_D-1_V\": \"V\", \"NWP1_00h_D-1_T\":\"T\"}, \n",
    "        inplace=True)\n",
    "\n",
    "    # Limpieza de datos: eliminar valores perdidos\n",
    "    only_na = X_train_cpy[~X_train_cpy.index.isin(X_train_cpy.dropna().index)]\n",
    "    X_train_cpy.dropna(inplace=True)\n",
    "    Y_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "\n",
    "    X_test_cpy.dropna(inplace=True)\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', ['time','U','V','month','hour','day_of_week'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.DerivedAttributesAdder()), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # aplciar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Random Forest\n",
    "    # Entrenamiento del modelo mediante k-fold cross validation\n",
    "    # Búsqueda de hiperparámetros mediante Randomized Search \n",
    "\n",
    "    forest_reg = RandomForestRegressor()\n",
    "\n",
    "    rf_random = RandomizedSearchCV(\n",
    "        estimator = forest_reg, \n",
    "        param_distributions = random_grid, \n",
    "        n_iter = 100, \n",
    "        cv = 3, \n",
    "        random_state=42, \n",
    "        scoring = cape_scorer,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "\n",
    "    rf_random.fit(X_train_pped, Y_train_cpy)\n",
    "    final_model = rf_random.best_estimator_\n",
    "\n",
    "    # guardamos modelo\n",
    "    models.append(joblib.dump(final_model, WF + 'rfrand'))\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    predictions = final_model.predict(X_test_pped)\n",
    "\n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True)  \n",
    "    print('Best score for {}: {}'.format(WF, -rf_random.best_score_))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('--------')\n",
    "\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"./Data/submission_rfrand.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 6: Regresión con Elastic Net (Ridge + Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.externals import joblib\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "# models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train_cpy.columns[3:-11]\n",
    "    cols_test = X_test_cpy.columns[3:-11]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_train)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U + X_train_cpy.NWP4_U)/4\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V + X_train_cpy.NWP4_V)/4\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = (X_train_cpy.NWP4_CLCT)\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = (X_test_cpy.NWP4_CLCT)\n",
    "\n",
    "    \n",
    "    \n",
    "    X_train_cpy = X_train_cpy[['Time','U','V','T','CLCT']]\n",
    "    X_test_cpy = X_test_cpy[['Time','U','V','T','CLCT']]\n",
    "\n",
    "    \n",
    "    ## Limpieza de datos: imputar valores perdidos en X_\n",
    "    #only_na = X_test_cpy[~X_test_cpy.ID.isin(X_test_cpy.dropna().ID)]\n",
    "    #X_test_cpy.dropna(inplace=True)\n",
    "    #Y_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "    #X_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', \n",
    "                                                     ['Time','U','V'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.DerivedAttributesAdder(add_time_feat=False, add_cyclic_feat=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Regresión polinomial con regularización Elastic Net\n",
    "    # Entrenamiento del modelo mediante k-fold cross validation\n",
    "    # Búsqueda de hiperparámetros mediante Gridsearch \n",
    "    \n",
    "    poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "    X_train_poly = poly_features.fit_transform(X_train_pped)\n",
    "    \n",
    "    param_grid = [{\n",
    "        'alpha'     : np.logspace(-3, -2, 1, 2, 3),\n",
    "        'l1_ratio'  : [0.00, 0.25, 0.50, 0.75, 1.0],\n",
    "        'tol'       : [0.00001, 0.0001, 0.001]\n",
    "    }]\n",
    "    \n",
    "                                               \n",
    "    eNet = ElasticNet(selection='random')\n",
    "    grid_search = GridSearchCV(\n",
    "        eNet, \n",
    "        param_grid, \n",
    "        cv=10,\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train_poly, Y_train_cpy)\n",
    "    final_model = grid_search.best_estimator_\n",
    "\n",
    "    # guardamos modelo\n",
    "    # models.append(joblib.dump(final_model, WF + '_eNet'))import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train_cpy.columns[3:-11]\n",
    "    cols_test = X_test_cpy.columns[3:-11]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_train)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U + X_train_cpy.NWP4_U)/4\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V + X_train_cpy.NWP4_V)/4\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = (X_train_cpy.NWP4_CLCT)\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = (X_test_cpy.NWP4_CLCT)\n",
    "\n",
    "    \n",
    "    \n",
    "    X_train_cpy = X_train_cpy[['Time','U','V','T','CLCT']]\n",
    "    X_test_cpy = X_test_cpy[['Time','U','V','T','CLCT']]\n",
    "\n",
    "    \n",
    "    ## Limpieza de datos: imputar valores perdidos en X_\n",
    "    #only_na = X_test_cpy[~X_test_cpy.ID.isin(X_test_cpy.dropna().ID)]\n",
    "    #X_test_cpy.dropna(inplace=True)\n",
    "    #Y_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "    #X_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', \n",
    "                                                     ['Time','U','V','T','CLCT'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.DerivedAttributesAdder(add_time_feat=False, add_cyclic_feat=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    X_test_poly = poly_features.fit_transform(X_test_pped)\n",
    "    predictions = final_model.predict(X_test_poly)\n",
    "\n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True)  \n",
    "    print('Best score for {}: {}'.format(WF, -grid_search.best_score_))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('--------')\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/submission_eNet.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 7: LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 6 candidates, totalling 42 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 out of  42 | elapsed:   13.9s finished\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-188144926e6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;31m# evaluación sobre el conjunto de test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[0mX_test_pped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_data_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_cpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[0mX_test_poly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpoly_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_pped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_poly\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1506\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1507\u001b[0m         \"\"\"\n\u001b[1;32m-> 1508\u001b[1;33m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1509\u001b[0m         combinations = self._combinations(n_features, self.degree,\n\u001b[0;32m   1510\u001b[0m                                           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteraction_only\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 562\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                     (type_err,\n\u001b[1;32m---> 60\u001b[1;33m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[0;32m     61\u001b[0m             )\n\u001b[0;32m     62\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "\n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train_cpy.columns[3:-11]\n",
    "    cols_test = X_test_cpy.columns[3:-11]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_train)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U + X_train_cpy.NWP4_U)/4\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V + X_train_cpy.NWP4_V)/4\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = (X_train_cpy.NWP4_CLCT)\n",
    "    X_train_cpy['inv_T'] = 1/(X_train_cpy['T'])\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = (X_test_cpy.NWP4_CLCT)\n",
    "    X_test_cpy['inv_T'] = 1/(X_test_cpy['T'])\n",
    "    \n",
    "    X_train_cpy = X_train_cpy[['Time','U','V','T','CLCT','inv_T']]\n",
    "    X_test_cpy = X_test_cpy[['Time','U','V','T','CLCT', 'inv_T']]\n",
    "\n",
    "    \n",
    "    ## Limpieza de datos: imputar valores perdidos en X_\n",
    "    #only_na = X_test_cpy[~X_test_cpy.ID.isin(X_test_cpy.dropna().ID)]\n",
    "    #X_test_cpy.dropna(inplace=True)\n",
    "    #Y_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "    #X_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', \n",
    "                                                     ['Time','U','V','T'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.DerivedAttributesAdder(add_time_feat=False, add_cyclic_feat=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Regresión polinomial utilizando CV con regularización tipo Ride\n",
    "    poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "    X_train_poly = poly_features.fit_transform(X_train_pped)\n",
    "    \n",
    "    param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "    lasso_reg = lm.Lasso()\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        lasso_reg, \n",
    "        param_grid, \n",
    "        cv=7,\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train_poly, Y_train_cpy)\n",
    "    final_model = grid_search.best_estimator_\n",
    "    \n",
    "    # guardamos modelo\n",
    "    models.append(joblib.dump(final_model, WF + '_lasso'))\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    X_test_poly = poly_features.fit_transform(X_test_pped)\n",
    "    predictions = final_model.predict(X_test_poly)\n",
    "    \n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('CAPE for {} is {}'.format(WF, -grid_search.best_score_))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/submission_lasso.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 8: Regresión robusta con RANSAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "    X_train_cpy = X_train_cpy[['Time','NWP1_00h_D-1_U','NWP1_00h_D-1_V','NWP1_00h_D-1_T']]\n",
    "    X_train_cpy.rename(\n",
    "        columns={\"Time\": \"time\", \"NWP1_00h_D-1_U\": \"U\", \"NWP1_00h_D-1_V\": \"V\", \"NWP1_00h_D-1_T\":\"T\"}, \n",
    "        inplace=True)\n",
    "\n",
    "    X_test_cpy = X_test_cpy[['Time','NWP1_00h_D-1_U','NWP1_00h_D-1_V','NWP1_00h_D-1_T']]\n",
    "    X_test_cpy.rename(\n",
    "        columns={\"Time\": \"time\", \"NWP1_00h_D-1_U\": \"U\", \"NWP1_00h_D-1_V\": \"V\", \"NWP1_00h_D-1_T\":\"T\"}, \n",
    "        inplace=True)\n",
    "\n",
    "    # Limpieza de datos: eliminar valores perdidos\n",
    "    only_na = X_train_cpy[~X_train_cpy.index.isin(X_train_cpy.dropna().index)]\n",
    "    X_train_cpy.dropna(inplace=True)\n",
    "    Y_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "\n",
    "    X_test_cpy.dropna(inplace=True)\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', ['time','U','V','T','w_dir'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.DerivedAttributesAdder(add_time_feat=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Regresión Robusta utilizando RANSAC\n",
    "    poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "    X_train_poly = poly_features.fit_transform(X_train_pped)\n",
    "    \n",
    "    ransac = RANSACRegressor(LinearRegression(), loss='absolute_loss')\n",
    "    param_grid = [\n",
    "        {\n",
    "            'max_trials': [100, 1000, 10000],\n",
    "            'min_samples': [10, 30, 50],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    grid_search_ransac = GridSearchCV(\n",
    "        ransac, \n",
    "        param_grid, \n",
    "        cv=5,\n",
    "        scoring=cape_scorer\n",
    "    )\n",
    "\n",
    "    grid_search_ransac.fit(X_train_poly, Y_train_cpy)\n",
    "    final_model = grid_search_ransac.best_estimator_\n",
    "    \n",
    "    # guardamos modelo\n",
    "    models.append(joblib.dump(final_model, WF + '_ransac'))\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    X_test_poly = poly_features.fit_transform(X_test_pped)\n",
    "    predictions = final_model.predict(X_test_poly)\n",
    "    \n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('CAPE for {} is {}'.format(WF, -grid_search_ransac.best_score_))\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"./Data/submission_ransac.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 10: MARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   60.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 12.4min\n",
      "[Parallel(n_jobs=-1)]: Done 210 out of 210 | elapsed: 15.0min finished\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\pyearth\\earth.py:813: FutureWarning:\n",
      "\n",
      "`rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\sklearn\\utils\\validation.py:933: FutureWarning:\n",
      "\n",
      "Passing attributes to check_is_fitted is deprecated and will be removed in 0.23. The attributes argument is ignored.\n",
      "\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\pyearth\\earth.py:1066: FutureWarning:\n",
      "\n",
      "`rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\sklearn\\utils\\validation.py:933: FutureWarning:\n",
      "\n",
      "Passing attributes to check_is_fitted is deprecated and will be removed in 0.23. The attributes argument is ignored.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPE for WF1 is 54.45958588792473\n",
      "Predictions for WF1 has been added to submission_df\n",
      "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   27.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 210 out of 210 | elapsed:  5.0min finished\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\pyearth\\earth.py:813: FutureWarning:\n",
      "\n",
      "`rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\sklearn\\utils\\validation.py:933: FutureWarning:\n",
      "\n",
      "Passing attributes to check_is_fitted is deprecated and will be removed in 0.23. The attributes argument is ignored.\n",
      "\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\pyearth\\earth.py:1066: FutureWarning:\n",
      "\n",
      "`rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\sklearn\\utils\\validation.py:933: FutureWarning:\n",
      "\n",
      "Passing attributes to check_is_fitted is deprecated and will be removed in 0.23. The attributes argument is ignored.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPE for WF2 is 43.96392106085858\n",
      "Predictions for WF2 has been added to submission_df\n",
      "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   58.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=-1)]: Done 210 out of 210 | elapsed: 14.5min finished\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\pyearth\\earth.py:813: FutureWarning:\n",
      "\n",
      "`rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\sklearn\\utils\\validation.py:933: FutureWarning:\n",
      "\n",
      "Passing attributes to check_is_fitted is deprecated and will be removed in 0.23. The attributes argument is ignored.\n",
      "\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\pyearth\\earth.py:1066: FutureWarning:\n",
      "\n",
      "`rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\sklearn\\utils\\validation.py:933: FutureWarning:\n",
      "\n",
      "Passing attributes to check_is_fitted is deprecated and will be removed in 0.23. The attributes argument is ignored.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPE for WF3 is 44.344418509673\n",
      "Predictions for WF3 has been added to submission_df\n",
      "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   44.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 210 out of 210 | elapsed:  6.8min finished\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\pyearth\\earth.py:813: FutureWarning:\n",
      "\n",
      "`rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\sklearn\\utils\\validation.py:933: FutureWarning:\n",
      "\n",
      "Passing attributes to check_is_fitted is deprecated and will be removed in 0.23. The attributes argument is ignored.\n",
      "\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\pyearth\\earth.py:1066: FutureWarning:\n",
      "\n",
      "`rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\sklearn\\utils\\validation.py:933: FutureWarning:\n",
      "\n",
      "Passing attributes to check_is_fitted is deprecated and will be removed in 0.23. The attributes argument is ignored.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPE for WF4 is 40.70388633516939\n",
      "Predictions for WF4 has been added to submission_df\n",
      "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   40.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 10.6min\n",
      "[Parallel(n_jobs=-1)]: Done 210 out of 210 | elapsed: 13.4min finished\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\pyearth\\earth.py:813: FutureWarning:\n",
      "\n",
      "`rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\sklearn\\utils\\validation.py:933: FutureWarning:\n",
      "\n",
      "Passing attributes to check_is_fitted is deprecated and will be removed in 0.23. The attributes argument is ignored.\n",
      "\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\pyearth\\earth.py:1066: FutureWarning:\n",
      "\n",
      "`rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\sklearn\\utils\\validation.py:933: FutureWarning:\n",
      "\n",
      "Passing attributes to check_is_fitted is deprecated and will be removed in 0.23. The attributes argument is ignored.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPE for WF5 is 53.24633113853439\n",
      "Predictions for WF5 has been added to submission_df\n",
      "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 210 out of 210 | elapsed:  1.4min finished\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\pyearth\\earth.py:813: FutureWarning:\n",
      "\n",
      "`rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\sklearn\\utils\\validation.py:933: FutureWarning:\n",
      "\n",
      "Passing attributes to check_is_fitted is deprecated and will be removed in 0.23. The attributes argument is ignored.\n",
      "\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\pyearth\\earth.py:1066: FutureWarning:\n",
      "\n",
      "`rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "\n",
      "C:\\Users\\Victor\\Anaconda3\\envs\\TFM-env\\lib\\site-packages\\sklearn\\utils\\validation.py:933: FutureWarning:\n",
      "\n",
      "Passing attributes to check_is_fitted is deprecated and will be removed in 0.23. The attributes argument is ignored.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPE for WF6 is 41.21084026442138\n",
      "Predictions for WF6 has been added to submission_df\n"
     ]
    }
   ],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model as lm\n",
    "from pyearth import Earth\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "    X_train_cpy = X_train_cpy[['Time','NWP1_00h_D-1_U','NWP1_00h_D-1_V','NWP1_00h_D-1_T']]\n",
    "    X_train_cpy.rename(\n",
    "        columns={\"Time\": \"time\", \"NWP1_00h_D-1_U\": \"U\", \"NWP1_00h_D-1_V\": \"V\", \"NWP1_00h_D-1_T\":\"T\"}, \n",
    "        inplace=True)\n",
    "\n",
    "    X_test_cpy = X_test_cpy[['Time','NWP1_00h_D-1_U','NWP1_00h_D-1_V','NWP1_00h_D-1_T']]\n",
    "    X_test_cpy.rename(\n",
    "        columns={\"Time\": \"time\", \"NWP1_00h_D-1_U\": \"U\", \"NWP1_00h_D-1_V\": \"V\", \"NWP1_00h_D-1_T\":\"T\"}, \n",
    "        inplace=True)\n",
    "\n",
    "    # Limpieza de datos: eliminar valores perdidos\n",
    "    only_na = X_train_cpy[~X_train_cpy.index.isin(X_train_cpy.dropna().index)]\n",
    "    X_train_cpy.dropna(inplace=True)\n",
    "    Y_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "\n",
    "    X_test_cpy.dropna(inplace=True)\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', ['time','U','V'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.DerivedAttributesAdder(add_time_feat=False, add_cyclic_feat=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: MARS utilizando py-earth\n",
    "    param_grid = [{'max_degree': [1,2,3], \n",
    "                   'allow_linear': [False, True], \n",
    "                   'penalty': [0.,1.,2.,3.,4.,5.,6.],\n",
    "                  }]\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        Earth(), \n",
    "        param_grid, \n",
    "        cv=5,\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train_pped, Y_train_cpy)\n",
    "    final_model = grid_search.best_estimator_\n",
    "    \n",
    "    # guardamos modelo\n",
    "    models.append(joblib.dump(final_model, WF + '_mars2'))\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    predictions = final_model.predict(X_test_pped)\n",
    "    \n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('CAPE for {} is {}'.format(WF, -grid_search.best_score_))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"./Data/submission_mars2.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
