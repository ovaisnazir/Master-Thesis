{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Common libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "from src.functions import data_import as dimp\n",
    "from src.functions import data_exploration as dexp\n",
    "from src.functions import data_transformation as dtr\n",
    "from src.functions import metric\n",
    "from src.functions import utils\n",
    "\n",
    "# Graphics\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import plotly as pty\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline=True)\n",
    "\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Save images \n",
    "DIR = \"../../TFM/reports/figures/\"\n",
    "WF = \"WF1\"\n",
    "IMAGES_PATH = os.path.join(DIR, WF)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore warnings (SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algunas pruebas con diferentes modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para establecer un punto de referencia o *benchmark*, probaremos distintos modelos sin profundizar mucho en el tuning de los hiperparámetros. El objetivo es paulatinamente mejorar el rendimiento de los modelos, a base de modificar las fases del proceso (EDA, Feature Engineering, Hyperparameter Tuning, ...).\n",
    "\n",
    "Elegiremos para conformar nuestro set de entrenamiento:\n",
    "* Un solo NWP.\n",
    "* Run de las 00h.\n",
    "* Día D-1.\n",
    "* Variables meteorológicas de partida `time`, `U`, `V` y `T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 29.94 MB\n",
      "Memory usage after optimization is: 7.72 MB\n",
      "Decreased by 74.2%\n",
      "Memory usage of dataframe is 29.26 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\quark\\documents\\mis cosas\\master\\asignaturas\\tfm\\git\\tfm\\src\\functions\\data_import.py:31: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in less\n",
      "\n",
      "c:\\users\\quark\\documents\\mis cosas\\master\\asignaturas\\tfm\\git\\tfm\\src\\functions\\data_import.py:33: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in less\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 19.47 MB\n",
      "Decreased by 33.5%\n",
      "Memory usage of dataframe is 0.57 MB\n",
      "Memory usage after optimization is: 0.21 MB\n",
      "Decreased by 62.5%\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "DATA_PATH = 'C:/Users/Quark/Documents/Mis Cosas/Master/Asignaturas/TFM/Git/TFM/data/raw/'\n",
    "X_train = dimp.import_data(os.path.join(DATA_PATH, 'X_train_v2.csv'))\n",
    "X_test = dimp.import_data(os.path.join(DATA_PATH, 'X_test_v2.csv'))\n",
    "Y_train = dimp.import_data(os.path.join(DATA_PATH, 'Y_train.csv'))\n",
    "\n",
    "X_train['Time'] = pd.to_datetime(X_train['Time'], format='%d/%m/%Y %H:%M')\n",
    "X_test['Time'] = pd.to_datetime(X_test['Time'], format='%d/%m/%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def input_missing_values(df, cols):\n",
    "    \n",
    "    regex = 'NWP(?P<NWP>\\d{1})_(?P<run>\\d{2}h)_(?P<fc_day>D\\W?\\d?)_(?P<weather_var>\\w{1,4})'\n",
    "    p = re.compile(regex)  \n",
    "    \n",
    "    NWP_met_vars_dict = {\n",
    "        '1': ['U','V','T'],\n",
    "        '2': ['U','V'],\n",
    "        '3': ['U','V','T'],\n",
    "        '4': ['U','V','CLCT']\n",
    "    }\n",
    "    \n",
    "    for col in reversed(cols):\n",
    "        m = p.match(col)\n",
    "        col_name = 'NWP' + m.group('NWP') + '_' +  m.group('run') + '_' + m.group('fc_day') + '_' + m.group('weather_var')\n",
    "\n",
    "        for key, value in NWP_met_vars_dict.items():\n",
    "            for i in value:\n",
    "                if m.group('NWP') == key and m.group('weather_var') == i:\n",
    "                    df['NWP'+ key + '_' + i] = df['NWP'+ key + '_' + i].fillna(df[col_name])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from metpy import calc\n",
    "from metpy.units import units\n",
    "\n",
    "# function to obtain the module of wind velocity\n",
    "get_wind_velmod = lambda x : float(calc.wind_speed(\n",
    "    x.U * units.meter/units.second, \n",
    "    x.V * units.meter/units.second\n",
    ").magnitude)\n",
    "\n",
    "# function to obtain the wind direction\n",
    "get_wind_dir = lambda x : float(calc.wind_direction(\n",
    "    x.U * units.meter/units.second, \n",
    "    x.V * units.meter/units.second, \n",
    "    convention=\"from\"\n",
    ").magnitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión con KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº features antes de la seleccion:  10\n",
      "Nº features después de la seleccion:  4\n",
      "R2 for WF1 is 0.8674065749207006\n",
      "Predictions for WF1 has been added to submission_df\n",
      "Nº features antes de la seleccion:  10\n",
      "Nº features después de la seleccion:  4\n",
      "R2 for WF2 is 0.9999998968002413\n",
      "Predictions for WF2 has been added to submission_df\n",
      "Nº features antes de la seleccion:  10\n",
      "Nº features después de la seleccion:  3\n",
      "R2 for WF3 is 0.999999901860768\n",
      "Predictions for WF3 has been added to submission_df\n",
      "Nº features antes de la seleccion:  10\n",
      "Nº features después de la seleccion:  4\n",
      "R2 for WF4 is 0.9999999035646198\n",
      "Predictions for WF4 has been added to submission_df\n",
      "Nº features antes de la seleccion:  10\n",
      "Nº features después de la seleccion:  4\n",
      "R2 for WF5 is 0.9999998978820784\n",
      "Predictions for WF5 has been added to submission_df\n",
      "Nº features antes de la seleccion:  10\n",
      "Nº features después de la seleccion:  3\n",
      "R2 for WF6 is 0.9999999174813817\n",
      "Predictions for WF6 has been added to submission_df\n",
      "********************************************\n",
      "Global score:  0.9779010154182983\n",
      "********************************************\n"
     ]
    }
   ],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import r2_score, median_absolute_error\n",
    "import hdbscan\n",
    "\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos y los scores de cada WF\n",
    "models = []\n",
    "scores_list = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "        \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "    \n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "    \n",
    "\n",
    "    X_train_cpy['U'] = X_train_cpy.NWP1_U\n",
    "    X_train_cpy['V'] = X_train_cpy.NWP1_V\n",
    "    X_train_cpy['T'] = X_train_cpy.NWP3_T\n",
    "    X_train_cpy['CLCT'] = X_train_cpy.NWP4_CLCT\n",
    "    \n",
    "    X_test_cpy['U'] = X_test_cpy.NWP1_U\n",
    "    X_test_cpy['V'] = X_test_cpy.NWP1_V\n",
    "    X_test_cpy['T'] = X_test_cpy.NWP3_T\n",
    "    X_test_cpy['CLCT'] = X_test_cpy.NWP4_CLCT\n",
    " \n",
    "    X_train_cpy = X_train_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    X_test_cpy = X_test_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    \n",
    "    # Hay 11 valores perdidos en la columna CLCT en X_test_cpy. Los imputamos\n",
    "    X_test_cpy.fillna(method='bfill', limit=11, inplace=True)\n",
    "\n",
    "    \n",
    "    ####### Limpiar outliers y valores anómalos #######\n",
    "    \n",
    "    # valores negativos en CLCT\n",
    "    X_train_cpy.loc[X_train_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    X_test_cpy.loc[X_test_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "\n",
    "    # añadir columna Production\n",
    "    X_train_cpy['Production'] = Y_train_cpy.to_list()\n",
    "    \n",
    "    # calcular módulo velocidad del viento\n",
    "    X_train_cpy['vel'] = X_train_cpy.apply(get_wind_velmod, axis=1)\n",
    "    \n",
    "    # formar matriz de datos \n",
    "    X1 = X_train_cpy['vel'].values.reshape(-1,1)\n",
    "    X2 = X_train_cpy['Production'].values.reshape(-1,1)\n",
    "    X = np.concatenate((X1,X2), axis=1)\n",
    "    \n",
    "    # algoritmo para detección de outliers\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=20).fit(X)\n",
    "    threshold = pd.Series(clusterer.outlier_scores_).quantile(0.96)\n",
    "    outliers = np.where(clusterer.outlier_scores_ > threshold)[0]\n",
    "    \n",
    "    # Eliminamos los registros outliers \n",
    "    X_train_cpy.drop(X_train_cpy.index[list(outliers)], inplace=True)\n",
    "    \n",
    "    # Eliminamos las observaciones corresp#ondientes de Y_train\n",
    "    Y_train_cpy = Y_train_cpy.loc[X_train_cpy['ID'].values - 1]\n",
    "    \n",
    "    # Eliminamos las columnas 'vel' y 'Production'\n",
    "    del X_train_cpy['vel']\n",
    "    del X_train_cpy['Production']\n",
    "    \n",
    "    ##################################\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', ['ID','Time','U','V','T','month','hour'])]\n",
    "    )\n",
    "    \n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.NewFeaturesAdder()), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('power_transf', PowerTransformer())\n",
    "    ])\n",
    "    \n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "    \n",
    "\n",
    "    print('Nº features antes de la seleccion: ', X_train_pped.shape[1])\n",
    "    \n",
    "    # seleccionar features más importantes mediante Random Forest\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    rf_reg = rf.fit(X_train_pped, Y_train_cpy)\n",
    "    sel = SelectFromModel(rf_reg, prefit=True, threshold='1.25*median')\n",
    "    X_train_pped = sel.transform(X_train_pped)\n",
    "    \n",
    "    print('Nº features después de la seleccion: ', X_train_pped.shape[1])\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización KNN: implementación en GridSearchCV\n",
    "    param_grid = [\n",
    "        {\n",
    "            'n_neighbors': list(range(1,50,2)),\n",
    "            'algorithm':['auto', 'kd_tree'],\n",
    "            'weights': ['uniform','distance'],\n",
    "            'p': [1,2]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    btscv = utils.BlockingTimeSeriesSplit(n_splits=5)\n",
    "    knn_reg = KNeighborsRegressor()\n",
    "    grid_search_knn = GridSearchCV(\n",
    "        knn_reg, \n",
    "        param_grid, \n",
    "        cv= btscv,\n",
    "        n_jobs=-1,\n",
    "        scoring=cape_scorer\n",
    "    )\n",
    "\n",
    "    grid_search_knn.fit(X_train_pped, Y_train_cpy)\n",
    "    \n",
    "    # Reentrenamos sin validación cruzada utilizando los mejores \n",
    "    # parámetros obtenidos con la validación cruzada\n",
    "    \n",
    "    knn_reg2 = KNeighborsRegressor(algorithm=grid_search_knn.best_params_['algorithm'],\n",
    "                                   n_neighbors=grid_search_knn.best_params_['n_neighbors'],\n",
    "                                   p=grid_search_knn.best_params_['p'], \n",
    "                                   weights=grid_search_knn.best_params_['weights'])\n",
    "    \n",
    "    ttreg = TransformedTargetRegressor(regressor=knn_reg2, \n",
    "                                       transformer=StandardScaler(), \n",
    "                                       check_inverse=False)\n",
    "\n",
    "    ttreg.fit(X_train_pped, Y_train_cpy)\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    X_test_pped = sel.transform(X_test_pped)\n",
    "    predictions = ttreg.predict(X_test_pped)\n",
    "    \n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('R2 for {} is {}'.format(WF, ttreg.score(X_train_pped, Y_train_cpy)))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    \n",
    "    scores_list.append(ttreg.score(X_train_pped, Y_train_cpy))\n",
    "    \n",
    "global_score = np.mean(scores_list)\n",
    "\n",
    "print('********************************************')\n",
    "print('Global score: ', global_score)\n",
    "print('********************************************')\n",
    "\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/submission_knn_time_only.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5989, 7)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method RegressorMixin.score of TransformedTargetRegressor(check_inverse=False, func=None, inverse_func=None,\n",
       "                           regressor=KNeighborsRegressor(algorithm='auto',\n",
       "                                                         leaf_size=30,\n",
       "                                                         metric='minkowski',\n",
       "                                                         metric_params=None,\n",
       "                                                         n_jobs=None,\n",
       "                                                         n_neighbors=28, p=1,\n",
       "                                                         weights='uniform'),\n",
       "                           transformer=PowerTransformer(copy=True,\n",
       "                                                        method='yeo-johnson',\n",
       "                                                        standardize=True))>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttreg.score(X_train_pped, Y_train_cpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_test_cpy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-bcd832d19b35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test_cpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Y_test_cpy' is not defined"
     ]
    }
   ],
   "source": [
    "np.array(Y_test_cpy).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Time</th>\n",
       "      <th>U</th>\n",
       "      <th>V</th>\n",
       "      <th>T</th>\n",
       "      <th>w_vel</th>\n",
       "      <th>w_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-05-01 01:00:00</td>\n",
       "      <td>-2.248047</td>\n",
       "      <td>-3.257812</td>\n",
       "      <td>286.50</td>\n",
       "      <td>3.958163</td>\n",
       "      <td>34.607537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-05-01 02:00:00</td>\n",
       "      <td>-2.433594</td>\n",
       "      <td>-1.446289</td>\n",
       "      <td>286.25</td>\n",
       "      <td>2.830924</td>\n",
       "      <td>59.276909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2018-05-01 03:00:00</td>\n",
       "      <td>3.365234</td>\n",
       "      <td>-3.060547</td>\n",
       "      <td>285.75</td>\n",
       "      <td>4.548818</td>\n",
       "      <td>312.285273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-05-01 04:00:00</td>\n",
       "      <td>3.707031</td>\n",
       "      <td>-6.218750</td>\n",
       "      <td>284.75</td>\n",
       "      <td>7.239816</td>\n",
       "      <td>329.200597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2018-05-01 05:00:00</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>-5.445312</td>\n",
       "      <td>284.50</td>\n",
       "      <td>6.647299</td>\n",
       "      <td>325.002463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6234</th>\n",
       "      <td>6235</td>\n",
       "      <td>2019-01-15 20:00:00</td>\n",
       "      <td>-2.546875</td>\n",
       "      <td>-7.687500</td>\n",
       "      <td>280.25</td>\n",
       "      <td>8.098409</td>\n",
       "      <td>18.330076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6235</th>\n",
       "      <td>6236</td>\n",
       "      <td>2019-01-15 21:00:00</td>\n",
       "      <td>-3.097656</td>\n",
       "      <td>-6.777344</td>\n",
       "      <td>279.75</td>\n",
       "      <td>7.451702</td>\n",
       "      <td>24.563269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6236</th>\n",
       "      <td>6237</td>\n",
       "      <td>2019-01-15 22:00:00</td>\n",
       "      <td>-3.261719</td>\n",
       "      <td>-5.277344</td>\n",
       "      <td>279.50</td>\n",
       "      <td>6.203964</td>\n",
       "      <td>31.718582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6237</th>\n",
       "      <td>6238</td>\n",
       "      <td>2019-01-15 23:00:00</td>\n",
       "      <td>-3.345703</td>\n",
       "      <td>-3.085938</td>\n",
       "      <td>279.25</td>\n",
       "      <td>4.551565</td>\n",
       "      <td>47.312845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6238</th>\n",
       "      <td>6239</td>\n",
       "      <td>2019-01-16 00:00:00</td>\n",
       "      <td>-3.933594</td>\n",
       "      <td>-1.296875</td>\n",
       "      <td>279.00</td>\n",
       "      <td>4.141865</td>\n",
       "      <td>71.753018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5177 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                Time         U         V       T     w_vel  \\\n",
       "0        1 2018-05-01 01:00:00 -2.248047 -3.257812  286.50  3.958163   \n",
       "1        2 2018-05-01 02:00:00 -2.433594 -1.446289  286.25  2.830924   \n",
       "2        3 2018-05-01 03:00:00  3.365234 -3.060547  285.75  4.548818   \n",
       "3        4 2018-05-01 04:00:00  3.707031 -6.218750  284.75  7.239816   \n",
       "4        5 2018-05-01 05:00:00  3.812500 -5.445312  284.50  6.647299   \n",
       "...    ...                 ...       ...       ...     ...       ...   \n",
       "6234  6235 2019-01-15 20:00:00 -2.546875 -7.687500  280.25  8.098409   \n",
       "6235  6236 2019-01-15 21:00:00 -3.097656 -6.777344  279.75  7.451702   \n",
       "6236  6237 2019-01-15 22:00:00 -3.261719 -5.277344  279.50  6.203964   \n",
       "6237  6238 2019-01-15 23:00:00 -3.345703 -3.085938  279.25  4.551565   \n",
       "6238  6239 2019-01-16 00:00:00 -3.933594 -1.296875  279.00  4.141865   \n",
       "\n",
       "           w_dir  \n",
       "0      34.607537  \n",
       "1      59.276909  \n",
       "2     312.285273  \n",
       "3     329.200597  \n",
       "4     325.002463  \n",
       "...          ...  \n",
       "6234   18.330076  \n",
       "6235   24.563269  \n",
       "6236   31.718582  \n",
       "6237   47.312845  \n",
       "6238   71.753018  \n",
       "\n",
       "[5177 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttreg = TransformedTargetRegressor(regressor=knn_reg(algorithm='auto',\n",
    "                                                    metric='minkowski',\n",
    "                                                    n_neighbors=28,\n",
    "                                                    p=1, weights='uniform'), func=np.log, inverse_func=np.exp)\n",
    "\n",
    "ttreg.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 1: Regresión polinomial con regularización *ridge*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'src.functions.data_transformation' has no attribute 'DerivedAttributesAdder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d4cfa2b98582>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;31m# definir pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     prepare_data_pipeline = Pipeline(steps=[\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[1;34m'attr_adder'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDerivedAttributesAdder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd_time_feat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_cyclic_feat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_vel_pot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[1;34m'pre_processing'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre_process\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[1;34m'std_scaler'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'src.functions.data_transformation' has no attribute 'DerivedAttributesAdder'"
     ]
    }
   ],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U + X_train_cpy.NWP4_U)/4\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V + X_train_cpy.NWP4_V)/4\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = (X_train_cpy.NWP4_CLCT)\n",
    "    X_train_cpy['T2'] = np.sqrt(X_train_cpy['T'])\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = (X_test_cpy.NWP4_CLCT)\n",
    "    X_test_cpy['T2'] = np.sqrt(X_test_cpy['T'])\n",
    "\n",
    "    \n",
    "    \n",
    "    X_train_cpy = X_train_cpy[['Time','U','V','T','CLCT','T2']]\n",
    "    X_test_cpy = X_test_cpy[['Time','U','V','T','CLCT','T2']]\n",
    "    \n",
    "    # Hay 11 valores perdidos en la columna CLCT en X_test_cpy. Los imputamos con la moda\n",
    "    X_test_cpy.fillna(method='bfill', limit=11, inplace=True)\n",
    "\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop',\n",
    "                                                     ['Time','U','V','T','CLCT','T2'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.DerivedAttributesAdder(add_time_feat=False, add_cyclic_feat=False, add_vel_pot=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Regresión polinomial utilizando CV con regularización tipo Ride\n",
    "    poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "    X_train_poly = poly_features.fit_transform(X_train_pped)\n",
    "    \n",
    "    rreg = lm.RidgeCV(alphas=np.logspace(-4, -3, 3, 4), store_cv_values=True)\n",
    "    rreg.fit(X_train_poly, Y_train_cpy)\n",
    "    \n",
    "    # guardamos modelo\n",
    "    models.append(joblib.dump(rreg, WF + '_rreg'))\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    X_test_poly = poly_features.fit_transform(X_test_pped)\n",
    "    predictions = rreg.predict(X_test_poly)\n",
    "    \n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('RMSE for {} is {}'.format(WF, np.mean(np.sqrt(rreg.cv_values_))))\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/submission_rreg.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_cpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nans = pd.DataFrame(data=np.isnan(X_test_pped).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 2: Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.externals import joblib\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 5)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train_cpy.columns[3:-11]\n",
    "    cols_test = X_test_cpy.columns[3:-11]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_train)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U + X_train_cpy.NWP4_U)/4\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V + X_train_cpy.NWP4_V)/4\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = (X_train_cpy.NWP4_CLCT)\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = (X_test_cpy.NWP4_CLCT)\n",
    "\n",
    "    \n",
    "    \n",
    "    X_train_cpy = X_train_cpy[['Time','U','V','T','CLCT']]\n",
    "    X_test_cpy = X_test_cpy[['Time','U','V','T','CLCT']]\n",
    "\n",
    "    \n",
    "    ## Limpieza de datos: imputar valores perdidos en X_\n",
    "    #only_na = X_test_cpy[~X_test_cpy.ID.isin(X_test_cpy.dropna().ID)]\n",
    "    #X_test_cpy.dropna(inplace=True)\n",
    "    #Y_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "    #X_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', \n",
    "                                                     ['Time','U','V','hour','month','day_of_week'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.DerivedAttributesAdder(add_time_feat=True, add_cyclic_feat=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('power_transf', PowerTransformer(method='yeo-johnson', standardize=True))\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "\n",
    "    # Modelización: Random Forest\n",
    "    # Entrenamiento del modelo mediante k-fold cross validation\n",
    "    # Búsqueda de hiperparámetros mediante Gridsearch \n",
    "    # param_grid = [\n",
    "    #    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    #    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "    #]\n",
    "\n",
    "    forest_reg = RandomForestRegressor()\n",
    "    grid_search = GridSearchCV(\n",
    "        forest_reg, \n",
    "        random_grid, \n",
    "        cv=5,\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=4\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train_pped, Y_train_cpy)\n",
    "    final_model = grid_search.best_estimator_\n",
    "\n",
    "    # guardamos modelo\n",
    "    #models.append(joblib.dump(final_model, WF))\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    predictions = final_model.predict(X_test_pped)\n",
    "\n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True)  \n",
    "    print('Best score for {}: {}'.format(WF, -grid_search.best_score_))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('--------')\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/submission_RF.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 3: SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quark\\.conda\\envs\\TFM-env\\lib\\site-packages\\numpy\\core\\_methods.py:122: RuntimeWarning:\n",
      "\n",
      "overflow encountered in reduce\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for WF1 is 0.8605183607331424\n",
      "Predictions for WF1 has been added to submission_df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quark\\.conda\\envs\\TFM-env\\lib\\site-packages\\numpy\\core\\_methods.py:122: RuntimeWarning:\n",
      "\n",
      "overflow encountered in reduce\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for WF2 is 0.8699449287575118\n",
      "Predictions for WF2 has been added to submission_df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quark\\.conda\\envs\\TFM-env\\lib\\site-packages\\numpy\\core\\_methods.py:122: RuntimeWarning:\n",
      "\n",
      "overflow encountered in reduce\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for WF3 is 0.8672894353453126\n",
      "Predictions for WF3 has been added to submission_df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quark\\.conda\\envs\\TFM-env\\lib\\site-packages\\numpy\\core\\_methods.py:122: RuntimeWarning:\n",
      "\n",
      "overflow encountered in reduce\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for WF4 is 0.9051895906311048\n",
      "Predictions for WF4 has been added to submission_df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quark\\.conda\\envs\\TFM-env\\lib\\site-packages\\numpy\\core\\_methods.py:122: RuntimeWarning:\n",
      "\n",
      "overflow encountered in reduce\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for WF5 is 0.8893119917044603\n",
      "Predictions for WF5 has been added to submission_df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quark\\.conda\\envs\\TFM-env\\lib\\site-packages\\numpy\\core\\_methods.py:122: RuntimeWarning:\n",
      "\n",
      "overflow encountered in reduce\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for WF6 is 0.8691731791308887\n",
      "Predictions for WF6 has been added to submission_df\n",
      "********************************************\n",
      "Global score:  0.7572046175247678\n",
      "********************************************\n"
     ]
    }
   ],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.externals import joblib\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "\n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = X_train_cpy.NWP1_U\n",
    "    X_train_cpy['V'] = X_train_cpy.NWP1_V\n",
    "    X_train_cpy['T'] = X_train_cpy.NWP3_T\n",
    "    X_train_cpy['CLCT'] = X_train_cpy.NWP4_CLCT\n",
    "    \n",
    "    X_test_cpy['U'] = X_test_cpy.NWP1_U\n",
    "    X_test_cpy['V'] = X_test_cpy.NWP1_V\n",
    "    X_test_cpy['T'] = X_test_cpy.NWP3_T\n",
    "    X_test_cpy['CLCT'] = X_test_cpy.NWP4_CLCT\n",
    " \n",
    "    X_train_cpy = X_train_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    X_test_cpy = X_test_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    \n",
    "    # Hay 11 valores perdidos en la columna CLCT en X_test_cpy. Los imputamos con la moda\n",
    "    X_test_cpy.fillna(method='bfill', limit=11, inplace=True)\n",
    "    \n",
    "    \n",
    "    ####### Limpiar outliers y valores anómalos #######\n",
    "    \n",
    "    # valores negativos en CLCT\n",
    "    X_train_cpy.loc[X_train_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    X_test_cpy.loc[X_test_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "\n",
    "    # añadir columna Production\n",
    "    X_train_cpy['Production'] = Y_train_cpy.to_list()\n",
    "    \n",
    "    # calcular módulo velocidad del viento\n",
    "    X_train_cpy['vel'] = X_train_cpy.apply(get_wind_velmod, axis=1)\n",
    "    \n",
    "    # formar matriz de datos \n",
    "    X1 = X_train_cpy['vel'].values.reshape(-1,1)\n",
    "    X2 = X_train_cpy['Production'].values.reshape(-1,1)\n",
    "    X = np.concatenate((X1,X2), axis=1)\n",
    "    \n",
    "    # algoritmo para detección de outliers\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=20).fit(X)\n",
    "    threshold = pd.Series(clusterer.outlier_scores_).quantile(0.96)\n",
    "    outliers = np.where(clusterer.outlier_scores_ > threshold)[0]\n",
    "    \n",
    "    # Eliminamos los registros outliers \n",
    "    X_train_cpy.drop(X_train_cpy.index[list(outliers)], inplace=True)\n",
    "    \n",
    "    # Eliminamos las observaciones corresp#ondientes de Y_train\n",
    "    Y_train_cpy = Y_train_cpy.loc[X_train_cpy['ID'].values - 1]\n",
    "    \n",
    "    # Eliminamos las columnas 'vel' y 'Production'\n",
    "    del X_train_cpy['vel']\n",
    "    del X_train_cpy['Production']\n",
    "    \n",
    "    ###################################\n",
    "    \n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', \n",
    "                                                     ['ID','Time','U','V','CLCT','month'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.NewFeaturesAdder(add_time_feat=True, add_cycl_feat=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', PowerTransformer())\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    print('Nº features antes de la seleccion: ', X_train_pped.shape[1])\n",
    "    # seleccionar features más importantes mediante Random Forest\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    rf_reg = rf.fit(X_train_pped, Y_train_cpy)\n",
    "    sel = SelectFromModel(rf_reg, prefit=True, threshold='1.75*median')\n",
    "    X_train_pped = sel.transform(X_train_pped)\n",
    "    \n",
    "    print('Nº features después de la seleccion: ', X_train_pped.shape[1])\n",
    "    '''\n",
    "  \n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Entrenamiento del modelo mediante k-fold cross validation\n",
    "    # Búsqueda de hiperparámetros mediante Gridsearch \n",
    "    param_grid = {\n",
    "        'kernel': ('linear', 'rbf','poly'), \n",
    "        'C':[0.01, 1, 0.1, 10],\n",
    "        'gamma': [0.00001, 0.001, 1],\n",
    "        'epsilon':[0.1,0.3,0.5]\n",
    "    }\n",
    "\n",
    "    svm_reg = SVR()\n",
    "    grid_search_svm = GridSearchCV(\n",
    "        svm_reg, \n",
    "        param_grid, \n",
    "        cv= utils.BlockingTimeSeriesSplit(n_splits=5),\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid_search_svm.fit(X_train_pped, Y_train_cpy)\n",
    "\n",
    "    # Reentrenamos sin validación cruzada utilizando los mejores \n",
    "    # parámetros obtenidos con la validación cruzada\n",
    "    \n",
    "    svm_reg2 = SVR(kernel = grid_search_svm.best_params_['kernel'],\n",
    "                  C = grid_search_svm.best_params_['C'],\n",
    "                  gamma = grid_search_svm.best_params_['gamma'],\n",
    "                  epsilon = grid_search_svm.best_params_['epsilon'])\n",
    "    \n",
    "    ttreg = TransformedTargetRegressor(regressor=svm_reg2, \n",
    "                                       transformer=PowerTransformer(), \n",
    "                                       check_inverse=False)\n",
    "\n",
    "    ttreg.fit(X_train_pped, Y_train_cpy)\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    # X_test_pped = sel.transform(X_test_pped)\n",
    "    predictions = ttreg.predict(X_test_pped)\n",
    "    \n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('R2 for {} is {}'.format(WF, ttreg.score(X_train_pped, Y_train_cpy)))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    \n",
    "    scores_list.append(ttreg.score(X_train_pped, Y_train_cpy))\n",
    "    \n",
    "global_score = np.mean(scores_list)\n",
    "\n",
    "print('********************************************')\n",
    "print('Global score: ', global_score)\n",
    "print('********************************************')\n",
    "\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/submission_SVR.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 4: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº features antes de la seleccion:  12\n",
      "Nº features después de la seleccion:  4\n",
      "[14:02:14] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:02:14] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "R2 for WF1 is 0.8956100729989427\n",
      "Predictions for WF1 has been added to submission_df\n",
      "Nº features antes de la seleccion:  12\n",
      "Nº features después de la seleccion:  1\n",
      "[14:02:55] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:02:55] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "R2 for WF2 is 0.8794353715432277\n",
      "Predictions for WF2 has been added to submission_df\n",
      "Nº features antes de la seleccion:  12\n",
      "Nº features después de la seleccion:  4\n",
      "[14:03:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:03:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "R2 for WF3 is 0.8792417854797664\n",
      "Predictions for WF3 has been added to submission_df\n",
      "Nº features antes de la seleccion:  12\n",
      "Nº features después de la seleccion:  1\n",
      "[14:04:30] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:04:30] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "R2 for WF4 is 0.8877146620651525\n",
      "Predictions for WF4 has been added to submission_df\n",
      "Nº features antes de la seleccion:  12\n",
      "Nº features después de la seleccion:  4\n",
      "[14:05:28] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:05:28] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "R2 for WF5 is 0.9270561316995192\n",
      "Predictions for WF5 has been added to submission_df\n",
      "Nº features antes de la seleccion:  12\n",
      "Nº features después de la seleccion:  3\n",
      "[14:06:13] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:06:13] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "R2 for WF6 is 0.8932442722107024\n",
      "Predictions for WF6 has been added to submission_df\n",
      "********************************************\n",
      "Global score:  0.8937170493328851\n",
      "********************************************\n"
     ]
    }
   ],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import hdbscan\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "scores_list = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U + X_train_cpy.NWP4_U)/4\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V + X_train_cpy.NWP4_V)/4\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = (X_train_cpy.NWP4_CLCT)\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = (X_test_cpy.NWP4_CLCT)\n",
    "\n",
    "    \n",
    "    X_train_cpy = X_train_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    X_test_cpy = X_test_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    \n",
    "    # Hay 11 valores perdidos en la columna CLCT en X_test_cpy. Los imputamos\n",
    "    X_test_cpy['CLCT'].fillna(method='bfill', limit=11, inplace=True)\n",
    "    \n",
    "     ####### Limpiar outliers y valores anómalos #######\n",
    "    \n",
    "    # valores negativos en CLCT\n",
    "    X_train_cpy.loc[X_train_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    X_test_cpy.loc[X_test_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "\n",
    "    # añadir columna Production\n",
    "    X_train_cpy['Production'] = Y_train_cpy.to_list()\n",
    "    \n",
    "    # calcular módulo velocidad del viento\n",
    "    X_train_cpy['vel'] = X_train_cpy.apply(get_wind_velmod, axis=1)\n",
    "    \n",
    "    # formar matriz de datos \n",
    "    X1 = X_train_cpy['vel'].values.reshape(-1,1)\n",
    "    X2 = X_train_cpy['Production'].values.reshape(-1,1)\n",
    "    X = np.concatenate((X1,X2), axis=1)\n",
    "    \n",
    "    # algoritmo para detección de outliers\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=20).fit(X)\n",
    "    threshold = pd.Series(clusterer.outlier_scores_).quantile(0.96)\n",
    "    outliers = np.where(clusterer.outlier_scores_ > threshold)[0]\n",
    "    \n",
    "    # Eliminamos los registros outliers \n",
    "    X_train_cpy.drop(X_train_cpy.index[list(outliers)], inplace=True)\n",
    "    \n",
    "    # Eliminamos las observaciones corresp#ondientes de Y_train\n",
    "    Y_train_cpy = Y_train_cpy.loc[X_train_cpy['ID'].values - 1]\n",
    "    \n",
    "    # Eliminamos las columnas 'vel' y 'Production'\n",
    "    del X_train_cpy['vel']\n",
    "    del X_train_cpy['Production']\n",
    "    \n",
    "    ###################################\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', ['ID','Time','U','V'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.NewFeaturesAdder()), \n",
    "        ('pre_processing', pre_process),\n",
    "    ])\n",
    "\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy, Y_train_cpy)\n",
    "    print('Nº features antes de la seleccion: ', X_train_pped.shape[1])\n",
    "    \n",
    "    # seleccionar features más importantes mediante Random Forest\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    rf_reg = rf.fit(X_train_pped, Y_train_cpy)\n",
    "    sel = SelectFromModel(rf_reg, prefit=True, threshold='1.75*median')\n",
    "    X_train_pped = sel.transform(X_train_pped)\n",
    "    \n",
    "    print('Nº features después de la seleccion: ', X_train_pped.shape[1])               \n",
    "\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Random Forest\n",
    "    # Entrenamiento del modelo mediante k-fold cross validation\n",
    "    # Búsqueda de hiperparámetros mediante Gridsearch \n",
    "    \n",
    "    param_grid = {   \n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5],\n",
    "    }\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=7)\n",
    "    xgb_reg = xgb.XGBRegressor()\n",
    "    grid_search_xgb = GridSearchCV(\n",
    "        xgb_reg, \n",
    "        param_grid, \n",
    "        cv=tscv,\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search_xgb.fit(X_train_pped, Y_train_cpy)\n",
    "    \n",
    "    reg = xgb.XGBRegressor(colsample_bytree = grid_search_xgb.best_params_['colsample_bytree'],\n",
    "                           gamma = grid_search_xgb.best_params_['gamma'],\n",
    "                           max_depth = grid_search_xgb.best_params_['max_depth'],\n",
    "                           min_child_weight = grid_search_xgb.best_params_['min_child_weight'],\n",
    "                           subsample = grid_search_xgb.best_params_['subsample'],\n",
    "                           random_state=42)\n",
    "                                       \n",
    "    reg.fit(X_train_pped, Y_train_cpy)\n",
    "    \n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    X_test_pped = sel.transform(X_test_pped)\n",
    "    predictions = reg.predict(X_test_pped)\n",
    "    \n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('R2 for {} is {}'.format(WF, reg.score(X_train_pped, Y_train_cpy)))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    \n",
    "    scores_list.append(reg.score(X_train_pped, Y_train_cpy))\n",
    "    \n",
    "global_score = np.mean(scores_list)\n",
    "\n",
    "print('********************************************')\n",
    "print('Global score: ', global_score)\n",
    "print('********************************************')\n",
    "\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/submission_xgb2.csv\", index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function ndarray.mean>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rg.named_steps.regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 5: Random Forest con validación Randomized Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U + X_train_cpy.NWP4_U)/4\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V + X_train_cpy.NWP4_V)/4\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = (X_train_cpy.NWP4_CLCT)\n",
    "    X_train_cpy['T2'] = np.sqrt(X_train_cpy['T'])\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = (X_test_cpy.NWP4_CLCT)\n",
    "    X_test_cpy['T2'] = np.sqrt(X_test_cpy['T'])\n",
    "\n",
    "    \n",
    "    X_train_cpy = X_train_cpy[['ID','Time','U','V','T','CLCT','T2']]\n",
    "    X_test_cpy = X_test_cpy[['ID','Time','U','V','T','CLCT','T2']]\n",
    "    \n",
    "    # Hay 11 valores perdidos en la columna CLCT en X_test_cpy. Los imputamos con la moda\n",
    "    X_test_cpy.fillna(method='bfill', limit=11, inplace=True)\n",
    "    \n",
    "    ####### Limpiar outliers #######\n",
    "    \n",
    "    # añadir columna Production\n",
    "    X_train_cpy['Production'] = Y_train_cpy.to_list()\n",
    "    \n",
    "    # calcular módulo velocidad del viento\n",
    "    X_train_cpy['vel'] = X_train_cpy.apply(get_wind_velmod, axis=1)\n",
    "    \n",
    "    # formar matriz de datos \n",
    "    X1 = X_train_cpy['vel'].values.reshape(-1,1)\n",
    "    X2 = X_train_cpy['Production'].values.reshape(-1,1)\n",
    "    X = np.concatenate((X1,X2), axis=1)\n",
    "    \n",
    "    # Definir el clasificador de outliers\n",
    "    clf = OneClassSVM(nu=0.17, gamma=0.06)\n",
    "    clf.fit(X)\n",
    "    \n",
    "    # Predección de outlier o inlier para cada punto\n",
    "    y_pred = clf.predict(X)\n",
    "    \n",
    "    # Añadirmos la columna 'oulier' con la predicción \n",
    "    X_train_cpy['outlier'] = y_pred.tolist()\n",
    "    \n",
    "    # Eliminamos los registros outliers \n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['outlier'] != -1]\n",
    "    \n",
    "    # Eliminamos las observaciones correspondientes de Y_train\n",
    "    Y_train_cpy = Y_train_cpy.loc[X_train_cpy['ID'].values - 1]\n",
    "    \n",
    "    # Eliminamos las columnas 'vel', 'outlier' y 'Production'\n",
    "    del X_train_cpy['vel']\n",
    "    del X_train_cpy['outlier']\n",
    "    del X_train_cpy['Production']\n",
    "    \n",
    "    ###################################\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', \n",
    "                                                     ['ID','Time','U','V','T2'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.NewFeaturesAdder(add_w_shear=False, add_time_feat=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "    # aplciar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Random Forest\n",
    "    # Entrenamiento del modelo mediante k-fold cross validation\n",
    "    # Búsqueda de hiperparámetros mediante Randomized Search \n",
    "\n",
    "    forest_reg = RandomForestRegressor()\n",
    "\n",
    "    rf_random = RandomizedSearchCV(\n",
    "        estimator = forest_reg, \n",
    "        param_distributions = random_grid, \n",
    "        n_iter = 100, \n",
    "        cv = 5, \n",
    "        random_state=42, \n",
    "        scoring = cape_scorer,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "\n",
    "    rf_random.fit(X_train_pped, Y_train_cpy)\n",
    "    final_model = rf_random.best_estimator_\n",
    "\n",
    "    # guardamos modelo\n",
    "    models.append(joblib.dump(final_model, WF + 'rfrand'))\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    predictions = final_model.predict(X_test_pped)\n",
    "\n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True)  \n",
    "    print('Best score for {}: {}'.format(WF, -rf_random.best_score_))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('--------')\n",
    "\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"./Data/submission_rfrand.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 6: Regresión con Elastic Net (Ridge + Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.externals import joblib\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "# models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U + X_train_cpy.NWP4_U)/4\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V + X_train_cpy.NWP4_V)/4\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = (X_train_cpy.NWP4_CLCT)\n",
    "    X_train_cpy['T2'] = np.sqrt(X_train_cpy['T'])\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = (X_test_cpy.NWP4_CLCT)\n",
    "    X_test_cpy['T2'] = np.sqrt(X_test_cpy['T'])\n",
    "\n",
    "    \n",
    "    X_train_cpy = X_train_cpy[['ID','Time','U','V','T','CLCT','T2']]\n",
    "    X_test_cpy = X_test_cpy[['ID','Time','U','V','T','CLCT','T2']]\n",
    "    \n",
    "    # Hay 11 valores perdidos en la columna CLCT en X_test_cpy. Los imputamos con la moda\n",
    "    X_test_cpy.fillna(method='bfill', limit=11, inplace=True)\n",
    "    \n",
    "    ####### Limpiar outliers #######\n",
    "    \n",
    "    # añadir columna Production\n",
    "    X_train_cpy['Production'] = Y_train_cpy.to_list()\n",
    "    \n",
    "    # calcular módulo velocidad del viento\n",
    "    X_train_cpy['vel'] = X_train_cpy.apply(get_wind_velmod, axis=1)\n",
    "    \n",
    "    # formar matriz de datos \n",
    "    X1 = X_train_cpy['vel'].values.reshape(-1,1)\n",
    "    X2 = X_train_cpy['Production'].values.reshape(-1,1)\n",
    "    X = np.concatenate((X1,X2), axis=1)\n",
    "    \n",
    "    # Definir el clasificador de outliers\n",
    "    clf = OneClassSVM(nu=0.17, gamma=0.06)\n",
    "    clf.fit(X)\n",
    "    \n",
    "    # Predección de outlier o inlier para cada punto\n",
    "    y_pred = clf.predict(X)\n",
    "    \n",
    "    # Añadirmos la columna 'oulier' con la predicción \n",
    "    X_train_cpy['outlier'] = y_pred.tolist()\n",
    "    \n",
    "    # Eliminamos los registros outliers \n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['outlier'] != -1]\n",
    "    \n",
    "    # Eliminamos las observaciones correspondientes de Y_train\n",
    "    Y_train_cpy = Y_train_cpy.loc[X_train_cpy['ID'].values - 1]\n",
    "    \n",
    "    # Eliminamos las columnas 'vel', 'outlier' y 'Production'\n",
    "    del X_train_cpy['vel']\n",
    "    del X_train_cpy['outlier']\n",
    "    del X_train_cpy['Production']\n",
    "    \n",
    "    ###################################\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', \n",
    "                                                     ['ID','Time','U','V','T2'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.NewFeaturesAdder(add_w_shear=False, add_time_feat=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Regresión polinomial con regularización Elastic Net\n",
    "    # Entrenamiento del modelo mediante k-fold cross validation\n",
    "    # Búsqueda de hiperparámetros mediante Gridsearch \n",
    "    \n",
    "    poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "    X_train_poly = poly_features.fit_transform(X_train_pped)\n",
    "    \n",
    "    param_grid = [{\n",
    "        'alpha'     : np.logspace(-3, -2, 1, 2, 3),\n",
    "        'l1_ratio'  : [0.00, 0.25, 0.50, 0.75, 1.0],\n",
    "        'tol'       : [0.00001, 0.0001, 0.001]\n",
    "    }]\n",
    "    \n",
    "                                               \n",
    "    eNet = ElasticNet(selection='random')\n",
    "    grid_search = GridSearchCV(\n",
    "        eNet, \n",
    "        param_grid, \n",
    "        cv=10,\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train_poly, Y_train_cpy)\n",
    "    final_model = grid_search.best_estimator_\n",
    "\n",
    "    # guardamos modelo\n",
    "    # models.append(joblib.dump(final_model, WF + '_eNet'))import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U + X_train_cpy.NWP4_U)/4\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V + X_train_cpy.NWP4_V)/4\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = (X_train_cpy.NWP4_CLCT)\n",
    "    X_train_cpy['T2'] = np.sqrt(X_train_cpy['T'])\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = (X_test_cpy.NWP4_CLCT)\n",
    "    X_test_cpy['T2'] = np.sqrt(X_test_cpy['T'])\n",
    "\n",
    "    \n",
    "    \n",
    "    X_train_cpy = X_train_cpy[['Time','U','V','T','CLCT','T2']]\n",
    "    X_test_cpy = X_test_cpy[['Time','U','V','T','CLCT','T2']]\n",
    "    \n",
    "    # Hay 11 valores perdidos en la columna CLCT en X_test_cpy. Los imputamos con la moda\n",
    "    X_test_cpy.fillna(method='bfill', limit=11, inplace=True)\n",
    "\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop',\n",
    "                                                     ['Time','U','V','T','CLCT','T2'])]\n",
    "    )\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    X_test_poly = poly_features.fit_transform(X_test_pped)\n",
    "    predictions = final_model.predict(X_test_poly)\n",
    "\n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True)  \n",
    "    print('Best score for {}: {}'.format(WF, -grid_search.best_score_))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('--------')\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/submission_eNet.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 7: LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "\n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train_cpy.columns[3:-11]\n",
    "    cols_test = X_test_cpy.columns[3:-11]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_train)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U + X_train_cpy.NWP4_U)/4\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V + X_train_cpy.NWP4_V)/4\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = (X_train_cpy.NWP4_CLCT)\n",
    "    X_train_cpy['inv_T'] = 1/(X_train_cpy['T'])\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = (X_test_cpy.NWP4_CLCT)\n",
    "    X_test_cpy['inv_T'] = 1/(X_test_cpy['T'])\n",
    "    \n",
    "    X_train_cpy = X_train_cpy[['Time','U','V','T','CLCT','inv_T']]\n",
    "    X_test_cpy = X_test_cpy[['Time','U','V','T','CLCT', 'inv_T']]\n",
    "\n",
    "    \n",
    "    ## Limpieza de datos: imputar valores perdidos en X_\n",
    "    #only_na = X_test_cpy[~X_test_cpy.ID.isin(X_test_cpy.dropna().ID)]\n",
    "    #X_test_cpy.dropna(inplace=True)\n",
    "    #Y_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "    #X_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', \n",
    "                                                     ['Time','U','V','T'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.DerivedAttributesAdder(add_time_feat=False, add_cyclic_feat=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Regresión polinomial utilizando CV con regularización tipo Ride\n",
    "    poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "    X_train_poly = poly_features.fit_transform(X_train_pped)\n",
    "    \n",
    "    param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "    lasso_reg = lm.Lasso()\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        lasso_reg, \n",
    "        param_grid, \n",
    "        cv=7,\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train_poly, Y_train_cpy)\n",
    "    final_model = grid_search.best_estimator_\n",
    "    \n",
    "    # guardamos modelo\n",
    "    models.append(joblib.dump(final_model, WF + '_lasso'))\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    X_test_poly = poly_features.fit_transform(X_test_pped)\n",
    "    predictions = final_model.predict(X_test_poly)\n",
    "    \n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('CAPE for {} is {}'.format(WF, -grid_search.best_score_))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/submission_lasso.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 8: Regresión robusta con RANSAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "    X_train_cpy = X_train_cpy[['Time','NWP1_00h_D-1_U','NWP1_00h_D-1_V','NWP1_00h_D-1_T']]\n",
    "    X_train_cpy.rename(\n",
    "        columns={\"Time\": \"time\", \"NWP1_00h_D-1_U\": \"U\", \"NWP1_00h_D-1_V\": \"V\", \"NWP1_00h_D-1_T\":\"T\"}, \n",
    "        inplace=True)\n",
    "\n",
    "    X_test_cpy = X_test_cpy[['Time','NWP1_00h_D-1_U','NWP1_00h_D-1_V','NWP1_00h_D-1_T']]\n",
    "    X_test_cpy.rename(\n",
    "        columns={\"Time\": \"time\", \"NWP1_00h_D-1_U\": \"U\", \"NWP1_00h_D-1_V\": \"V\", \"NWP1_00h_D-1_T\":\"T\"}, \n",
    "        inplace=True)\n",
    "\n",
    "    # Limpieza de datos: eliminar valores perdidos\n",
    "    only_na = X_train_cpy[~X_train_cpy.index.isin(X_train_cpy.dropna().index)]\n",
    "    X_train_cpy.dropna(inplace=True)\n",
    "    Y_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "\n",
    "    X_test_cpy.dropna(inplace=True)\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', ['time','U','V','T','w_dir'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.DerivedAttributesAdder(add_time_feat=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Regresión Robusta utilizando RANSAC\n",
    "    poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "    X_train_poly = poly_features.fit_transform(X_train_pped)\n",
    "    \n",
    "    ransac = RANSACRegressor(LinearRegression(), loss='absolute_loss')\n",
    "    param_grid = [\n",
    "        {\n",
    "            'max_trials': [100, 1000, 10000],\n",
    "            'min_samples': [10, 30, 50],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    grid_search_ransac = GridSearchCV(\n",
    "        ransac, \n",
    "        param_grid, \n",
    "        cv=5,\n",
    "        scoring=cape_scorer\n",
    "    )\n",
    "\n",
    "    grid_search_ransac.fit(X_train_poly, Y_train_cpy)\n",
    "    final_model = grid_search_ransac.best_estimator_\n",
    "    \n",
    "    # guardamos modelo\n",
    "    models.append(joblib.dump(final_model, WF + '_ransac'))\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    X_test_poly = poly_features.fit_transform(X_test_pped)\n",
    "    predictions = final_model.predict(X_test_poly)\n",
    "    \n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('CAPE for {} is {}'.format(WF, -grid_search_ransac.best_score_))\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"./Data/submission_ransac.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 10: MARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº features después de la seleccion:  4\n",
      "R2 for WF1 is 0.8236895292320006\n",
      "Predictions for WF1 has been added to submission_df\n",
      "Nº features después de la seleccion:  4\n",
      "R2 for WF2 is 0.8754899687597073\n",
      "Predictions for WF2 has been added to submission_df\n",
      "Nº features después de la seleccion:  4\n",
      "R2 for WF3 is 0.841930576034218\n",
      "Predictions for WF3 has been added to submission_df\n",
      "Nº features después de la seleccion:  4\n",
      "R2 for WF4 is 0.88094477423363\n",
      "Predictions for WF4 has been added to submission_df\n",
      "Nº features después de la seleccion:  4\n",
      "R2 for WF5 is 0.8943742198669061\n",
      "Predictions for WF5 has been added to submission_df\n",
      "Nº features después de la seleccion:  4\n",
      "R2 for WF6 is 0.8615119003145766\n",
      "Predictions for WF6 has been added to submission_df\n",
      "********************************************\n",
      "Global score:  0.8629901614068398\n",
      "********************************************\n"
     ]
    }
   ],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model as lm\n",
    "from pyearth import Earth\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "np.warnings.filterwarnings('ignore')\n",
    "import hdbscan\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "scores_list = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U + X_train_cpy.NWP4_U)/4\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V + X_train_cpy.NWP4_V)/4\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = (X_train_cpy.NWP4_CLCT)\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = (X_test_cpy.NWP4_CLCT)\n",
    "\n",
    "    \n",
    "    X_train_cpy = X_train_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    X_test_cpy = X_test_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    \n",
    "    # Hay 11 valores perdidos en la columna CLCT en X_test_cpy. Los imputamos con la moda\n",
    "    X_test_cpy.fillna(method='bfill', limit=11, inplace=True)\n",
    "    \n",
    "     ####### Limpiar outliers y valores anómalos #######\n",
    "    \n",
    "    # valores negativos en CLCT\n",
    "    X_train_cpy.loc[X_train_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    X_test_cpy.loc[X_test_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "\n",
    "    # añadir columna Production\n",
    "    X_train_cpy['Production'] = Y_train_cpy.to_list()\n",
    "    \n",
    "    # calcular módulo velocidad del viento\n",
    "    X_train_cpy['vel'] = X_train_cpy.apply(get_wind_velmod, axis=1)\n",
    "    \n",
    "    # formar matriz de datos \n",
    "    X1 = X_train_cpy['vel'].values.reshape(-1,1)\n",
    "    X2 = X_train_cpy['Production'].values.reshape(-1,1)\n",
    "    X = np.concatenate((X1,X2), axis=1)\n",
    "    \n",
    "    # algoritmo para detección de outliers\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=20).fit(X)\n",
    "    threshold = pd.Series(clusterer.outlier_scores_).quantile(0.96)\n",
    "    outliers = np.where(clusterer.outlier_scores_ > threshold)[0]\n",
    "    \n",
    "    # Eliminamos los registros outliers \n",
    "    X_train_cpy.drop(X_train_cpy.index[list(outliers)], inplace=True)\n",
    "    \n",
    "    # Eliminamos las observaciones corresp#ondientes de Y_train\n",
    "    Y_train_cpy = Y_train_cpy.loc[X_train_cpy['ID'].values - 1]\n",
    "    \n",
    "    # Eliminamos las columnas 'vel' y 'Production'\n",
    "    del X_train_cpy['vel']\n",
    "    del X_train_cpy['Production']\n",
    "    \n",
    "    ###################################\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', ['ID','Time','U','V','month','CLCT'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.NewFeaturesAdder(add_cycl_feat=False, add_time_feat=True)), \n",
    "        ('pre_processing', pre_process)\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy, Y_train_cpy)\n",
    "    # print('Nº features antes de la seleccion: ', X_train_pped.shape[1])\n",
    "    \n",
    "    # seleccionar features más importantes mediante Random Forest\n",
    "    # rf = RandomForestRegressor(random_state=42)\n",
    "    # rf_reg = rf.fit(X_train_pped, Y_train_cpy)\n",
    "    # sel = SelectFromModel(rf_reg, prefit=True, threshold='1.75*median')\n",
    "    # X_train_pped = sel.transform(X_train_pped)    \n",
    "    # print('Nº features después de la seleccion: ', X_train_pped.shape[1])\n",
    "    \n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: MARS utilizando py-earth\n",
    "    param_grid = [{'max_degree': [1,2,3], \n",
    "                   'allow_linear': [False, True], \n",
    "                   'penalty': [0.,1.,2.,3.,4.,5.,6.],\n",
    "                   # 'endspan_alpha': list(np.arange(0,1,0.1)),\n",
    "                   #'minspan_alpha': list(np.arange(0,1,0.1))\n",
    "                  }]\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=7)\n",
    "    grid_search = GridSearchCV(\n",
    "        Earth(), \n",
    "        param_grid, \n",
    "        cv=tscv,\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train_pped, Y_train_cpy)\n",
    "    \n",
    "    # Reentrenamos sin validación cruzada utilizando los mejores \n",
    "    # parámetros obtenidos con la validación cruzada\n",
    "    \n",
    "    mars = Earth(max_degree=grid_search.best_params_['max_degree'],\n",
    "                 allow_linear=grid_search.best_params_['allow_linear'],\n",
    "                 penalty=grid_search.best_params_['penalty'],\n",
    "                 # endspan=grid_search.best_params_['endspan'],\n",
    "                 # minspan=grid_search.best_params_['minspan']\n",
    "                )\n",
    "    \n",
    "    #ttreg = TransformedTargetRegressor(regressor=mars, transformer=StandardScaler(), check_inverse=False)\n",
    "    \n",
    "    mars.fit(X_train_pped, Y_train_cpy)\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    # X_test_pped = sel.transform(X_test_pped)\n",
    "    predictions = mars.predict(X_test_pped)\n",
    "    \n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('R2 for {} is {}'.format(WF, mars.rsq_))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    \n",
    "    scores_list.append(mars.rsq_)\n",
    "    \n",
    "global_score = np.mean(scores_list)\n",
    "\n",
    "print('********************************************')\n",
    "print('Global score: ', global_score)\n",
    "print('********************************************')\n",
    "\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/submission_mars.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
